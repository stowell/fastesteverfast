<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<title>Fastest Ever Fast</title>
<link rel="stylesheet" type="text/css" href="mobile.css" media="screen, handheld" />
<link rel="stylesheet" type="text/css" href="og.css" media="screen  and (min-width: 33em)" />
</head>
<body>
<h1 id="fastest-ever-fast">Fastest Ever Fast</h1>
<p>After taking a handful of embarrassingly slow systems and making them twice or a hundred times less embarrassing, I told myself I had in fact quit the latency business and started writing some ostentatiously unoptimized code. &#39;We&#39;re all constrained by the speed of light,&#39; a definitely-grizzled-and-presumably-experienced dude once told me. He sounded tired. Not wanting to be taken in, to play the shell game, I stopped caring for a time.</p><p>But, having taken a break from vim, sampling profilers, daytime high-fructose corn syrup, it&#39;s pulling me back in. The particular game that we keep playing: doing as little IO as possible. What I want to capture here, on a single page, are tricks that come up repeatedly when trying to shuffle as few bytes as possible.</p><p><em>Italicized apology: I have spent most of my time in the world of milliseconds. If you measure microseconds, nanoseconds, clock cycles, or do anything that can be called &#39;real-time&#39; (not slow), then you will not find many surprises here.</em></p>
<h3 id="perfect-hashing">Perfect Hashing</h3>
<p>If there&#39;s a simpler name for this please call me and tell me: when you have a small number of objects/records/tuples/things (say, fewer than 4 billion) and all you ever want to do is check if two of the things are equal, you can give them all unique integer IDs. Anyone who has spent a few minutes around me at the golden happy hour will know of my fondness for unsigned 32-bit integers.</p><p>Besides making equality checks a single machine instruction, perfect hashing has good knock-on benefits. You can index into arrays using the hash code itself, saving a lookup. You can use bitmaps as sets (or plug the hash codes right into a library like sparsehash). If certain records appear much more often than others (say in the logs of a hits-driven business like music streaming), you can give the popular records low IDs and compress entire columns much more densely than you could by gzipping the original bytes.</p><p>The trick of course comes in generating the mapping from things to integers and back. I&#39;ve found it useful to hash the records, grouping them into buckets by taking the low or high 16 bits, then iterating over each group while keeping a counter. Only ever appending to the mapping will also save you headaches.</p>
<p class="copyright">ds, april thirtieth, two thousand sixteen</p>
</body>
</html>